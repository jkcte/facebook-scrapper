# Project Update — 2025-09-18

## What’s in place

- HAR interpreter `har_extract_fb_serp.py`
  - Parses Facebook HARs for:
    - GraphQL search result entries (serpResponse → story → message → text)
    - Generic GraphQL message objects
    - Route bundle payloads (ajax/bulk-route-definitions → exports.meta.title / rootView.props.headerTitle)
  - Outputs a fixed schema CSV: `text, timestamp_utc, timestamp_manila, permalink, friendly_name, doc_id, entry_index, mode`
  - Robust body decoding with charset guess (`chardet`) and base64 handling
  - Strips Facebook JSON guard (`for (;;);`) automatically

- Live interceptor `facebookScrape.py`
  - Automates browser (Chrome/Edge/Opera GX profile) to capture network
  - Produces a session folder with artifacts and network capture (HAR/JSON)

- Initial orchestrator `runScript.py`
  - Current interface: `python runScript.py <facebook_link> <output_csv> [--filter <kw>]`
  - Runs the scraper, collects HAR(s), then calls the interpreter

## Documentation

- Root `README.md` updated with:
  - Quickstart parser usage
  - One-shot orchestration via `runScript.py` (current and planned interface)
  - Detailed explanation of extraction modes and troubleshooting

## Recent validations

- Successfully parsed a debug HAR:
  - Command: `python har_extract_fb_serp.py "debug_graphql_cap\session_1758131046768\network.har" --out opc_results.csv`
  - Exit code: 0 — produced output CSV

## Next steps (queued)

1. Refactor `runScript.py` to use flags: `--link`, `--output`, `--filter`
2. Harden `har_extract_fb_serp.py` further with more test cases and counters
3. Design Reddit ingestion + sentiment + clustering pipeline
4. Reporting layer (CSV + HTML/Markdown)
5. Optional Facebook posting (token-based, with dry-run)

## Notes

- All examples verified on Windows with PowerShell. Use `python` or `py` as available.
- Ensure `pip install chardet` in your active environment before running the interpreter.
